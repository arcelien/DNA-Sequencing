{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of DNA Sequences to identify invasive species with semi-supervised training\n",
    "\n",
    "## Machine Learning at Berkeley Research Project\n",
    "\n",
    "### Background\n",
    "\n",
    "We attempt to solve the classification problem of identifying invasive species given binary labels and a DNA dataset from the island of Morea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method\n",
    "\n",
    "We use a semi-supervised method to take advantage of unlabeled DNA, which makes up over 80% of the dataset.\n",
    "\n",
    "First, we load the DNA sequences and compute a similarity matrix which is easier for us to work with. We can add unlabeled data when creating this matrix, because it is not dependent on labels.\n",
    "\n",
    "We are losing infomation about the actual DNA sequences if we do this, so we can try a different method based on reading the ATCG bases in the future (RNNs?)\n",
    "\n",
    "Then, we can use a clustering algorithm like SVC to perform classification on the processed matrix. We are doing supervised learning, so we simply discard the unlabeled rows of our data matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn as sk\n",
    "from __future__ import division\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import math\n",
    "\n",
    "from tqdm import trange\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use pandas to read the excel sheet, and then extract features and convert the data to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the excel sheet \n",
    "df = pd.read_excel('./BioCode for Machine Learning.xlsx')\n",
    "\n",
    "# Read in the labels\n",
    "cls = df['Classification']\n",
    "\n",
    "# Read the DNA sequences, which are strings comprised of the letters ATCG\n",
    "seq = df['Aligned Sequence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we're working with are snippets of DNA a few hundred bases long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'ACTTTATATTTTCTATTTGGAACATGAGCTGGAATAGTAGGAACATCTCTAAGA---ATTTTAATTCGTGCAGAACTTGGACATCCA---GGAGCTTTA------ATTGGAGATGATCAAATTTATAATGTAATTGTAACAGCTCATGCTTTTGTAATAATTTTTTTTATAGTAATACCTATTATAATTGGAGGATTTGGAAATTGATTAGTTCCTTTAATACTTGGAGCTCCTGATATAGCTTTTCCTCGAATAAACAATATAAGTTTTTGATTATTACCACCTTCTTTAACTTTATTATTAGTAAGAAGAATAGTTGAAAATGGAGCAGGAACTGGTTGAACAGTTTACCCTCCTCTTTCTGCTAGAATTGCACATGGAGGTGCATCTGTTGATTTAGCTATTTTTTCTCTTCATTTAGCTGGTATATCATCAATTTTAGGAGCAGTAAATTTTATTACTACAGTAATTAATATACGATCAAATGGAATTTCATATGATCGTATACCTTTATTTGTATGATCAGTTGTAATTACAGCTTTATTATTATTATTATCTTTACCTGTATTAGCAGGAGCTATTACTATACTTTTAACAGATCGAAATCTAAATACCTCATTTTTTGATCCTGCTGGAGGAGGAGATCCTATTTTATATCAACATTTATTT--------------------------------'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the labels below. As we can see, the labels are very messy. We can only use the values of `Indigenous`, `Invasive`, or `NaN` for supervised training. However, because most unlabeled data points still have an associated DNA sequence, we can still use them in an unsupervised pre-training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Indigenous\n",
      "1     Indigenous\n",
      "2              0\n",
      "3            NaN\n",
      "4              0\n",
      "5     Indigenous\n",
      "6            NaN\n",
      "7            NaN\n",
      "8     Indigenous\n",
      "9     Indigenous\n",
      "10             0\n",
      "11             0\n",
      "12             0\n",
      "13             0\n",
      "14      Invasive\n",
      "15    Introduced\n",
      "16    Introduced\n",
      "17             0\n",
      "18    Introduced\n",
      "19    Introduced\n",
      "Name: Classification, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(cls[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, some species don't even have associated DNA sequences. We have to discard these before we proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffles the data (to make sure)\n",
    "#cls = cls.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DNA data to numpy array, and convert NaNs to Nones\n",
    "seq = np.array(seq.fillna('None'))\n",
    "\n",
    "# Create a binary filter to eliminate invalid DNA sequences\n",
    "valid_idx = np.array([i for i in range(len(seq)) if seq[i] != 'None'])\n",
    "\n",
    "# Apply the filter\n",
    "valid_seq = seq[valid_idx]\n",
    "cls_valid = cls[valid_idx]\n",
    "cls_valid = np.array(cls_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we process the DNA sequences by converting the string of bases into an array of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4639, 4639, 4639)\n"
     ]
    }
   ],
   "source": [
    "# Seperate string into individual characters\n",
    "seq_arrays = [np.array([i for i in s]) for s in valid_seq]\n",
    "\n",
    "mat_size = len(seq_arrays)\n",
    "\n",
    "print(len(valid_seq), len(cls_valid), mat_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a similarity matrix, which is a pairwise comparison of DNA sequences and determining the percentage of base pairs that are the same.\n",
    "\n",
    "Because the DNA sequences have been pre-aligned, we can expect this to be mostly accurate and close to the true similiarity values. In some places, the DNA sequences have a '-' character where the base was not read correctly, or missed. We ignore these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = -np.ones((mat_size, mat_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precomputer no dashes\n",
    "dashes = []\n",
    "for i in range(mat_size):\n",
    "    dashes.append(seq_arrays[i] != '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4639/4639 [03:40<00:00, 21.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# this will take a few minutes\n",
    "for i in trange(mat_size):\n",
    "    # clean up bad data\n",
    "    a = seq_arrays[i]\n",
    "    # iterate over DNA sequences and figure out the match\n",
    "    for j in range(i):\n",
    "        b = seq_arrays[j]\n",
    "        match = (a==b)\n",
    "        valid = (dashes[i] * dashes[j])\n",
    "        sim_mat[i,j] = np.mean(match[valid])\n",
    "        sim_mat[j,i] = sim_mat[i,j]\n",
    "    sim_mat[i,i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.75342466,  0.78713629, ...,  0.80980392,\n",
       "         0.77777778,  0.85692542],\n",
       "       [ 0.75342466,  1.        ,  0.75616438, ...,  0.75342466,\n",
       "         0.70136986,  0.74794521],\n",
       "       [ 0.78713629,  0.75616438,  1.        , ...,  0.74852652,\n",
       "         0.75957121,  0.77641654],\n",
       "       ..., \n",
       "       [ 0.80980392,  0.75342466,  0.74852652, ...,  1.        ,\n",
       "         0.72745098,  0.77647059],\n",
       "       [ 0.77777778,  0.70136986,  0.75957121, ...,  0.72745098,\n",
       "         1.        ,  0.78234399],\n",
       "       [ 0.85692542,  0.74794521,  0.77641654, ...,  0.77647059,\n",
       "         0.78234399,  1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mat = sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4639, 4639), (4639, 4639))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_mat.shape, valid_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity matrix is very big (100mb+), so we try PCA/SVD to extract the most useful features from the largest singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 27s, sys: 2min 8s, total: 6min 36s\n",
      "Wall time: 57.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "u,s,v = np.linalg.svd(valid_mat, full_matrices=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank / number of singular values we pick is a hyperparameter. We run the dimension reduction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.00000000e+00,   4.00000000e+00,   1.00000000e+01,\n",
       "          3.15000000e+02,   1.64744000e+05,   2.13539440e+07,\n",
       "          7.11000000e+02,   4.23000000e+02,   1.53000000e+02,\n",
       "          1.60000000e+01]),\n",
       " array([-0.01478794, -0.01193264, -0.00907734, -0.00622204, -0.00336673,\n",
       "        -0.00051143,  0.00234387,  0.00519918,  0.00805448,  0.01090978,\n",
       "         0.01376508]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEDCAYAAAAx/aOOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0b17b7a5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rank = 1000\n",
    "approx_1000 = u[:,:rank].dot(np.diag(s[:rank])).dot(v[:rank])\n",
    "errors = ((approx_1000 - valid_mat)/valid_mat)\n",
    "plt.hist(errors.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx = approx_1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a set of features from our pre-training step, and we're ready to run supervised training. Before we start, we need to first remove data that don't have valid labels. We can't use them anymore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'Indigenous', u'Indigenous', u'Indigenous', ..., u'Indigenous',\n",
       "       u'Introduced', u'Invasive'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels = ['Introduced', 'Invasive', 'Indigenous']\n",
    "labeled_cls = [label in valid_labels for label in cls_valid]\n",
    "#labeled_cls = (valid_labels[labeled_cls] == 'Indigenous').astype(int)\n",
    "\n",
    "# Create a filter telling us which points are valid to use for supervised training\n",
    "labeled_cls = np.array(labeled_cls)\n",
    "#print(len(labeled_cls))\n",
    "cls_valid[labeled_cls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 36% of the data have associated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34425522741970255"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(labeled_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1684, 4639), (1684,))\n"
     ]
    }
   ],
   "source": [
    "# apply the filter over our features and labels\n",
    "supervised_X = approx[labeled_cls]\n",
    "full_supervised_X = valid_mat[labeled_cls]\n",
    "supervised_y = cls_valid[labeled_cls]\n",
    "\n",
    "supervised_y = (supervised_y == 'Indigenous').astype(int)\n",
    "\n",
    "print(supervised_X.shape, supervised_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ..., 1 0 0]\n",
      "[u'Indigenous' u'Indigenous' u'Indigenous' ..., u'Indigenous' u'Introduced'\n",
      " u'Invasive']\n"
     ]
    }
   ],
   "source": [
    "print supervised_y\n",
    "print cls_valid[labeled_cls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we run Support Vector Clustering (SVC). We shuffle the data first, and then split our data into testing and training splits.\n",
    "\n",
    "There is somewhat large variance inbetween runs, so we take the average for a more accurate score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximated similarity matrix: \n",
      "\n",
      "([0.97626112759643913, 0.97032640949554894], 0.97329376854599403)\n",
      "Full similarity matrix: \n",
      "\n",
      "([0.97032640949554894, 0.96142433234421365], 0.96587537091988129)\n"
     ]
    }
   ],
   "source": [
    "# 10/18\n",
    "\n",
    "def test_and_score(supervised_X, supervised_y, c=1e15):\n",
    "    avg_score = []\n",
    "    for _ in range(2):\n",
    "        res_mat_shuff, cls_valid_shuff = sk.utils.shuffle(supervised_X, supervised_y, random_state=0)\n",
    "\n",
    "        cls_train, cls_test, res_train, res_test = train_test_split(cls_valid_shuff, res_mat_shuff, test_size=0.2)\n",
    "\n",
    "        # print(len(cls_train), len(cls_test))\n",
    "\n",
    "        X = res_train\n",
    "        y = cls_train\n",
    "        X_test = res_test\n",
    "\n",
    "        clf = SVC(C=c, kernel='poly', degree=2, coef0=0)\n",
    "\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        predict = clf.predict(X_test)\n",
    "\n",
    "        # print(predict == np.array(cls_test))\n",
    "\n",
    "        score = np.mean((predict == np.array(cls_test))*1)\n",
    "        avg_score.append(score)\n",
    "\n",
    "    print(avg_score, np.mean(avg_score))\n",
    "\n",
    "print 'Approximated similarity matrix: \\n'\n",
    "test_and_score(supervised_X, supervised_y)\n",
    "print 'Full similarity matrix: \\n'\n",
    "test_and_score(full_supervised_X, supervised_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the results are very competitive.\n",
    "\n",
    "Now, we use cross validation method get another take on our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximated similarity matrix: \n",
      "\n",
      "[array([ 0.99644128,  0.98576512,  0.98932384,  0.99644128,  1.        ,\n",
      "        0.98928571])]\n",
      "('mean scores:', [0.99287620742247074])\n",
      "\n",
      "Full similarity matrix: \n",
      "\n",
      "[array([ 0.99644128,  0.98576512,  0.98932384,  0.99644128,  1.        ,\n",
      "        0.98928571])]\n",
      "('mean scores:', [0.99287620742247074])\n"
     ]
    }
   ],
   "source": [
    "# cross validation method\n",
    "# SVM\n",
    "\n",
    "# tricks: shuffling data, cross validation, balanced classes, hyperparam tuning\n",
    "\n",
    "def cv_test_and_score(supervised_X, supervised_y, c=1e14):\n",
    "    scores = []\n",
    "    param_vals = []\n",
    "    \n",
    "    # shuffle the data\n",
    "    res_mat_shuff, cls_valid_shuff = sk.utils.shuffle(supervised_X, supervised_y, random_state=0)\n",
    "\n",
    "    c = 10*c\n",
    "    clf = SVC(C=c,kernel='poly', degree=2, coef0=0) #, gamma=i)\n",
    "\n",
    "    score = sk.cross_validation.cross_val_score(clf, res_mat_shuff, cls_valid_shuff, cv=6) #, n_jobs=-1)\n",
    "    # print('Prediction accuracy:', np.mean((prediction == np.array(cls_test))*1))\n",
    "    #Coefficients used by the classifier\n",
    "\n",
    "    scores.append(score)\n",
    "    param_vals.append(i)\n",
    "\n",
    "    print(scores)\n",
    "\n",
    "    mn_scores = [np.mean(score) for score in scores]\n",
    "\n",
    "    print('mean scores:', mn_scores)\n",
    "\n",
    "print 'Approximated similarity matrix: \\n'\n",
    "cv_test_and_score(supervised_X, supervised_y)\n",
    "print '\\nFull similarity matrix: \\n'\n",
    "cv_test_and_score(full_supervised_X, supervised_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice that we included our testing data when creating the similiarity matrix, because we first create the matrix and then separate the data into train and test sets. This is somewhat unsatisfying, and very anonying if we want to do on the fly predictions. We have to recompute the simliarity matrix every time.\n",
    "\n",
    "We now try excluding the test data from computing the similarity matrix. Instead, we can compute the values for the test data afterwards. We then also need to project the similiarity values for the test data to the SVD space, before we can run SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}