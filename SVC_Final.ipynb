{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of DNA Sequencing to Identify Invasive Species\n",
    "\n",
    "## Machine Learning at Berkeley\n",
    "\n",
    "We attempt to solve the classification problem of identifying invasive species given binary labels and a DNA dataset from the island of Morea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a semi-supervised method to take advantage of unlabeled DNA, which makes up over 80% of the dataset.\n",
    "\n",
    "First, we load the DNA sequences and compute a similarity matrix which is easier for us to work with. We can add unlabeled data when creating this matrix, because it is not dependent on labels.\n",
    "\n",
    "We are losing infomation about the actual DNA sequences if we do this, so we can try a different method based on reading the ATCG bases in the future (RNNs?)\n",
    "\n",
    "Then, we can use a clustering algorithm like SVC to perform classification on the processed matrix. We are doing supervised learning, so we simply discard the unlabeled rows of our data matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn as sk\n",
    "from __future__ import division\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import math\n",
    "\n",
    "from tqdm import trange\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use pandas to read the excel sheet, and then extract features and convert the data to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the excel sheet \n",
    "df = pd.read_excel('./BioCode for Machine Learning.xlsx')\n",
    "\n",
    "# Read in the labels\n",
    "cls = df['Classification']\n",
    "\n",
    "# Convert the label (Indigenous/Non-native/Invasive) into a binary label\n",
    "cls_valid = (cls == 'Indigenous')*1\n",
    "# cls_valid = cls_binary[valid_samples]\n",
    "\n",
    "\n",
    "# Read the DNA sequences, which are strings comprised of the letters ATCG\n",
    "seq = df['Aligned Sequence']\n",
    "\n",
    "# Convert data to numpy array\n",
    "seq = np.array(seq.fillna('None'))\n",
    "\n",
    "# Seperate string into individual characters\n",
    "seq_arrays = [np.array([i for i in s]) for s in seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a similarity matrix, which is a pairwise comparison of DNA sequences and determining the percentage of base pairs that are the same.\n",
    "\n",
    "Because the DNA sequences have been pre-aligned, we can expect this to be mostly accurate and close to the true similiarity values. In some places, the DNA sequences have a '-' character where the base was not read correctly, or missed. We ignore these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4401/4877 [04:19<00:58,  8.14it/s]"
     ]
    }
   ],
   "source": [
    "mat_size = len(seq_arrays)\n",
    "sim_mat = -np.ones((mat_size, mat_size))\n",
    "\n",
    "# this will take a few minutes\n",
    "for i in trange(mat_size):\n",
    "    # clean up bad data\n",
    "    if seq[i] != 'None':\n",
    "        a = seq_arrays[i]\n",
    "        # iterate over DNA sequences and figure out the match\n",
    "        for j in range(i):\n",
    "            if seq[j] != 'None':\n",
    "                b = seq_arrays[j]\n",
    "                match = (a==b)\n",
    "                valid = ((a != '-')* (b != '-')) == 1\n",
    "                sim_mat[i,j] = np.mean(match[valid])\n",
    "                sim_mat[j,i] = sim_mat[i,j]\n",
    "        sim_mat[i,i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now figure out valid rows of the similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_idx = np.array([i for i in range(len(seq)) if seq[i] != 'None'])\n",
    "valid_mat = sim_mat[valid_idx][:, valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_mat.shape, valid_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity matrix is very big (100mb+), so we try PCA/SVD to extract the most useful features from the largest singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "u,s,v = np.linalg.svd(valid_mat, full_matrices=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank / number of singular values we pick is a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rank = 1000\n",
    "approx_1000 = u[:,:rank].dot(np.diag(s[:rank])).dot(v[:rank])\n",
    "errors = ((approx_1000 - valid_mat)/valid_mat)\n",
    "plt.hist(errors.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the unlabeled data from our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_idx = list(valid_idx)\n",
    "valid_mat_idx = [valid_idx.index(i) for i in np.where(valid_samples)[0] if i in valid_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat = approx[valid_mat_idx]\n",
    "res_mat = residues[valid_mat_idx]\n",
    "print(mat.shape, res_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we run Support Vector Clustering. We shuffle the data first, and then split our data into testing and training splits.\n",
    "\n",
    "There is somewhat large variance inbetween runs, so we take the average for a more accurate score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10/18\n",
    "\n",
    "c = 1e15\n",
    "avg_score = []\n",
    "\n",
    "for _ in range(2):\n",
    "    res_mat_shuff, cls_valid_shuff = sk.utils.shuffle(approx[valid_mat_idx], cls_valid, random_state=0)\n",
    "\n",
    "    cls_train, cls_test, res_train, res_test = train_test_split(cls_valid_shuff, res_mat_shuff, test_size=0.2)\n",
    "\n",
    "    print(len(cls_train), len(cls_test))\n",
    "\n",
    "    X = res_train\n",
    "    y = cls_train\n",
    "    X_test = res_test\n",
    "\n",
    "    clf = SVC(C=c, kernel='poly', degree=2, coef0=0)\n",
    "\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    predict = clf.predict(X_test)\n",
    "\n",
    "    # print(predict == np.array(cls_test))\n",
    "\n",
    "    score = np.mean((predict == np.array(cls_test))*1)\n",
    "    avg_score.append(score)\n",
    "\n",
    "print(avg_score, np.mean(avg_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the results are very competitive.\n",
    "\n",
    "Now, we use cross validation method get another take on our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross validation method\n",
    "# SVM\n",
    "\n",
    "# tricks: shuffling data, cross validation, balanced classes, hyperparam tuning\n",
    "\n",
    "scores = []\n",
    "param_vals = []\n",
    "c = 1e14\n",
    "\n",
    "# shuffle the data\n",
    "res_mat_shuff, cls_valid_shuff = sk.utils.shuffle(res_mat, cls_valid, random_state=0)\n",
    "\n",
    "c = 10*c\n",
    "clf = SVC(C=c,kernel='poly', degree=2, coef0=0) #, gamma=i)\n",
    "\n",
    "score = sk.cross_validation.cross_val_score(clf, res_mat_shuff, cls_valid_shuff, cv=6) #, n_jobs=-1)\n",
    "# print('Prediction accuracy:', np.mean((prediction == np.array(cls_test))*1))\n",
    "#Coefficients used by the classifier\n",
    "\n",
    "scores.append(score)\n",
    "param_vals.append(i)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "mn_scores = [np.mean(score) for score in scores]\n",
    "\n",
    "print('mean scores:', mn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice that we included our testing data when creating the similiarity matrix, because we first create the matrix and then separate the data into train and test sets. This is somewhat unsatisfying, and very anonying if we want to do on the fly predictions. We have to recompute the simliarity matrix every time.\n",
    "\n",
    "We now try excluding the test data from computing the similarity matrix. Instead, we can compute the values for the test data afterwards. We then also need to project the similiarity values for the test data to the SVD space, before we can run SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
