{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of DNA Sequences to identify invasive species with semi-supervised training\n",
    "\n",
    "## Machine Learning at Berkeley Research Project\n",
    "\n",
    "### Background\n",
    "\n",
    "We attempt to solve the classification problem of identifying invasive species given binary labels and a DNA dataset from the island of Morea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method\n",
    "\n",
    "We separate training and testing data completely. Otherwise, same as `SVC_Final.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn as sk\n",
    "from __future__ import division\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import math\n",
    "\n",
    "from tqdm import trange\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use pandas to read the excel sheet, and then extract features and convert the data to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the excel sheet \n",
    "df = pd.read_excel('./BioCode for Machine Learning Updated.xlsx')\n",
    "\n",
    "# Read in the labels\n",
    "cls = df['Classification']\n",
    "\n",
    "# Read the DNA sequences, which are strings comprised of the letters ATCG\n",
    "seq = df['Aligned Sequence']\n",
    "\n",
    "species = df['NCBI_Genus_species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we're working with are snippets of DNA a few hundred bases long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACATTATACTTCATATTTGGAGGATGAGCCGGAATAGTAGGAACCTCGTTAAGA---ATACTTATTCGCGCAGAACTTAATCAACCA---GGATCCCTT------ATTGGAGATGATCAAATTTATAATGTTATTGTTACAGCCCACGCATTTGTTATAATTTTCTTTATAGTTATACCAATCTTGATTGGAGGGTTTGGAAATTGATTAGTACCTCTAATATTAGGAGCACCAGATATAGCATTCCCACGAATAAATAATATAAGATTCTGATTATTACCCCCATCACTCTCATTATTATTAACCAGTAGATTAGTCGAAAGAGGAGCTGGTACTGGTTGAACTGTATACCCACCCTTAGCTAGAGGGTTAGCCCATGCTGGTGCATCTGTTGATCTTGCAATCTTTTCTCTACACTTAGCAGGTGTTTCCTCTATTTTAGGAGCAGTTAATTTCATTTCAACAACAATCAATATAAAACCAATAAATATAACATCAGACCGAATCCCTTTATTTGTATGAGCTGTAGCAATCACAGCTTTACTTCTATTATTATCCCTACCAGTGCTTGCAGGAGCAATTACTATATTATTAACAGACCGAAACCTAAATACATCATTTTTTGACCCAGCTGGCGGGGGGGATCCTATTCTCTATCAACATTTATTT--------------------------------'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4459\n",
      "<class 'pandas.core.series.Series'> <class 'pandas.core.series.Series'> <class 'pandas.core.series.Series'>\n",
      "764\n"
     ]
    }
   ],
   "source": [
    "unique = set()\n",
    "valid = []\n",
    "for i in species:\n",
    "    if i not in unique:\n",
    "        valid.append(True)\n",
    "        unique.add(i)\n",
    "    else:\n",
    "        valid.append(False)\n",
    "        \n",
    "print(len(valid))\n",
    "print(type(cls), type(seq), type(species))\n",
    "cls, seq, species = cls[valid], seq[valid], species[valid]\n",
    "\n",
    "print(len(cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the labels below. As we can see, the labels are very messy. We can only use the values of `Indigenous`, `Invasive`, or `NaN` for supervised training. However, because most unlabeled data points still have an associated DNA sequence, we can still use them in an unsupervised pre-training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            NaN\n",
      "2       Invasive\n",
      "3       Invasive\n",
      "5            NaN\n",
      "10    Introduced\n",
      "11    Introduced\n",
      "12    Introduced\n",
      "18    Introduced\n",
      "29           NaN\n",
      "33           NaN\n",
      "34    Introduced\n",
      "35           NaN\n",
      "37           NaN\n",
      "38           NaN\n",
      "39           NaN\n",
      "41           NaN\n",
      "42           NaN\n",
      "43           NaN\n",
      "47           NaN\n",
      "48           NaN\n",
      "Name: Classification, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(cls[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, some species don't even have associated DNA sequences. We have to discard these before we proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffles the data (to make sure)\n",
    "#cls = cls.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert DNA data to numpy array, and convert NaNs to Nones\n",
    "\n",
    "seq = np.array(seq.fillna('None'))\n",
    "\n",
    "# Create a binary filter to eliminate invalid DNA sequences\n",
    "valid_idx = np.array([i for i in range(len(seq)) if seq[i] != 'None'])\n",
    "\n",
    "# Apply the filter\n",
    "valid_seq = seq[valid_idx]\n",
    "cls_valid = cls[valid_idx]\n",
    "cls_valid = np.array(cls_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we process the DNA sequences by converting the string of bases into an array of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACATTATACTTCATATTTGGAGGATGAGCCGGAATAGTAGGAACCTCGTTAAGA---ATACTTATTCGCGCAGAACTTAATCAACCA---GGATCCCTT------ATTGGAGATGATCAAATTTATAATGTTATTGTTACAGCCCACGCATTTGTTATAATTTTCTTTATAGTTATACCAATCTTGATTGGAGGGTTTGGAAATTGATTAGTACCTCTAATATTAGGAGCACCAGATATAGCATTCCCACGAATAAATAATATAAGATTCTGATTATTACCCCCATCACTCTCATTATTATTAACCAGTAGATTAGTCGAAAGAGGAGCTGGTACTGGTTGAACTGTATACCCACCCTTAGCTAGAGGGTTAGCCCATGCTGGTGCATCTGTTGATCTTGCAATCTTTTCTCTACACTTAGCAGGTGTTTCCTCTATTTTAGGAGCAGTTAATTTCATTTCAACAACAATCAATATAAAACCAATAAATATAACATCAGACCGAATCCCTTTATTTGTATGAGCTGTAGCAATCACAGCTTTACTTCTATTATTATCCCTACCAGTGCTTGCAGGAGCAATTACTATATTATTAACAGACCGAAACCTAAATACATCATTTTTTGACCCAGCTGGCGGGGGGGATCCTATTCTCTATCAACATTTATTT--------------------------------'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764 764\n"
     ]
    }
   ],
   "source": [
    "# Seperate string into individual bases. So, each value in the array is a base. Stored in list\n",
    "seq_arr = [np.array([i for i in s]) for s in valid_seq]\n",
    "\n",
    "#seq_mtx = len(seq_arr)\n",
    "\n",
    "print(len(valid_seq), len(cls_valid)) #, seq_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "valid_labels = ['Introduced', 'Invasive', 'Indigenous']\n",
    "cls_labeled = [label in valid_labels for label in cls_valid]\n",
    "#labeled_cls = (valid_labels[labeled_cls] == 'Indigenous').astype(int)\n",
    "\n",
    "# Create a filter telling us which points are valid to use for supervised training\n",
    "cls_labeled = np.array(cls_labeled)\n",
    "#print(len(labeled_cls))\n",
    "unshuffled_labels = cls_valid[cls_labeled]\n",
    "\n",
    "print(type(seq_arr))\n",
    "\n",
    "seq_data = [i for i, validity in zip(seq_arr, cls_labeled) if validity]\n",
    "\n",
    "print(type(seq_data))\n",
    "print(len(seq_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "27\n",
      "27 27\n"
     ]
    }
   ],
   "source": [
    "cls_valid_shuff, seq_data_shuff = sk.utils.shuffle(unshuffled_labels, seq_data, random_state=1337)\n",
    "\n",
    "print(len(cls_valid_shuff))\n",
    "\n",
    "# cls_train, cls_test, res_train, res_test = train_test_split(cls_valid_shuff, res_mat_shuff, test_size=0.2)\n",
    "train_test_split = int(len(cls_valid_shuff)*0.5)\n",
    "\n",
    "print(train_test_split)\n",
    "\n",
    "cls_test, cls_train = cls_valid_shuff[:train_test_split], cls_valid_shuff[train_test_split:]\n",
    "\n",
    "print(len(cls_test), len(cls_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "710\n"
     ]
    }
   ],
   "source": [
    "cls_unlabeled = 1 - cls_labeled\n",
    "\n",
    "seq_data_unlabeled = [i for i, validity in zip(seq_arr, cls_unlabeled) if validity]\n",
    "\n",
    "print(len(seq_data_unlabeled))\n",
    "\n",
    "# print(np.mean(unlabled_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "710\n"
     ]
    }
   ],
   "source": [
    "num_unlabeled = np.sum(cls_unlabeled)\n",
    "print(num_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a similarity matrix, which is a pairwise comparison of DNA sequences and determining the percentage of base pairs that are the same.\n",
    "\n",
    "Because the DNA sequences have been pre-aligned, we can expect this to be mostly accurate and close to the true similiarity values. In some places, the DNA sequences have a '-' character where the base was not read correctly, or missed. We ignore these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(737, 701)\n",
      "737\n"
     ]
    }
   ],
   "source": [
    "sim_train = np.vstack((seq_data_unlabeled, seq_data_shuff[train_test_split:]))\n",
    "print(sim_train.shape)\n",
    "\n",
    "mat_size = len(sim_train)\n",
    "print(mat_size)\n",
    "\n",
    "sim_mat_train = -np.ones((mat_size, mat_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Precompute no dashes\n",
    "dashes_train = []\n",
    "for i in range(mat_size):\n",
    "    dashes_train.append(sim_train[i] != '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(737, 737)\n"
     ]
    }
   ],
   "source": [
    "print(sim_mat_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 737/737 [00:06<00:00, 117.54it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert False\n",
    "    np.load('online_sim_mat_train.npy')\n",
    "\n",
    "except:\n",
    "    # this will take a few minutes\n",
    "    for i in trange(mat_size):\n",
    "        # clean up bad data\n",
    "        a = sim_train[i]\n",
    "        # iterate over DNA sequences and figure out the match\n",
    "        for j in range(i):\n",
    "            b = sim_train[j]\n",
    "            match = (a==b)\n",
    "            valid = (dashes_train[i] * dashes_train[j])\n",
    "            sim_mat_train[i,j] = np.mean(match[valid])\n",
    "            sim_mat_train[j,i] = sim_mat_train[i,j]\n",
    "        sim_mat_train[i,i] = 1\n",
    "    np.save('online_sim_mat_train.npy', sim_mat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_mat_train = sim_mat_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((737, 737), (737, 737))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_mat_train.shape, valid_mat_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity matrix is very big (100mb+), so we try PCA/SVD to extract the most useful features from the largest singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# u,s,v = np.linalg.svd(valid_mat, full_matrices=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank / number of singular values we pick is a hyperparameter. We run the dimension reduction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rank = 1000\n",
    "# approx_1000 = u[:,:rank].dot(np.diag(s[:rank])).dot(v[:rank])\n",
    "# errors = ((approx_1000 - valid_mat)/valid_mat)\n",
    "# plt.hist(errors.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a set of features from our pre-training step, and we're ready to run supervised training. Before we start, we need to first remove data that don't have valid labels. We can't use them anymore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 36% of the data have associated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.070680628272251314"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cls_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the filter over our features and labels\n",
    "# supervised_X = approx[labeled_cls]\n",
    "supervised_X = valid_mat_train[num_unlabeled:]\n",
    "# supervised_y = cls_valid[labeled_cls]\n",
    "\n",
    "supervised_y_train = (cls_train == 'Indigenous').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we run Support Vector Clustering (SVC). We shuffle the data first, and then split our data into testing and training splits.\n",
    "\n",
    "There is somewhat large variance inbetween runs, so we take the average for a more accurate score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1000000000000000.0, cache_size=200, class_weight=None, coef0=0,\n",
       "  decision_function_shape='ovr', degree=2, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10/18\n",
    "\n",
    "c = 1e15\n",
    "\n",
    "avg_score = []\n",
    "\n",
    "\n",
    "# for _ in range(2):\n",
    "#     res_mat_shuff, cls_valid_shuff = sk.utils.shuffle(supervised_X, supervised_y, random_state=0)\n",
    "\n",
    "#     cls_train, cls_test, res_train, res_test = train_test_split(cls_valid_shuff, res_mat_shuff, test_size=0.2)\n",
    "\n",
    "# print(len(cls_train), len(cls_test))\n",
    "\n",
    "X = supervised_X\n",
    "y = supervised_y_train #.reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "clf = SVC(C=c, kernel='poly', degree=2, coef0=0)\n",
    "\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data projection\n",
    "\n",
    "We need to get the test data, which is currently still DNA bases, into sim matrix rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 701)\n",
      "(737, 27)\n"
     ]
    }
   ],
   "source": [
    "#sim_test = np.vstack((seq_arr_unlabeled, seq_arr_shuff[:train_test_split]))\n",
    "sim_test = np.array(seq_data_shuff[:train_test_split])\n",
    "\n",
    "sim_mat_test = -np.ones((mat_size, len(sim_test)))\n",
    "\n",
    "print(sim_test.shape)\n",
    "print(sim_mat_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Precompute no dashes\n",
    "dashes_test = []\n",
    "for j in range(len(sim_test)):\n",
    "    dashes_test.append(sim_test[j] != '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(len(dashes_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 737/737 [00:00<00:00, 1614.66it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert False\n",
    "    np.load('online_sim_mat_test.npy')\n",
    "except:\n",
    "    # this will take a few minutes\n",
    "    for i in trange(mat_size):\n",
    "        # clean up bad data\n",
    "        a = sim_train[i]\n",
    "        # iterate over DNA sequences and figure out the match\n",
    "        for j in range(len(sim_test)):\n",
    "            b = sim_test[j]\n",
    "            match = (a==b)\n",
    "            valid = (dashes_train[i] * dashes_test[j])\n",
    "            sim_mat_test[i,j] = np.mean(match[valid])\n",
    "            # sim_mat_test[j,i] = sim_mat_test[i,j]\n",
    "        # sim_mat_test[i,i] = 1\n",
    "    np.save('online_sim_mat_test.npy', sim_mat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.54059829  0.80131363  0.74733638 ...,  0.77083333  0.78538813\n",
      "   0.74668874]\n",
      " [ 0.56410256  0.9589491   0.75190259 ...,  0.79166667  0.78082192\n",
      "   0.75662252]\n",
      " [ 0.56410256  0.80131363  0.75494673 ...,  0.78125     0.80974125\n",
      "   0.79304636]\n",
      " ..., \n",
      " [ 0.5982906   0.75907591  0.72588055 ...,  0.74842767  0.74272588\n",
      "   0.70333333]\n",
      " [ 0.60042735  0.75205255  0.79147641 ...,  0.83541667  0.85388128\n",
      "   0.78807947]\n",
      " [ 0.58974359  0.75369458  0.76712329 ...,  0.81458333  0.84018265\n",
      "   0.79801325]]\n"
     ]
    }
   ],
   "source": [
    "print(sim_mat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(737, 27)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_mat_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27,) (27,)\n"
     ]
    }
   ],
   "source": [
    "supervised_y_test = (cls_test == 'Indigenous').astype(int)\n",
    "\n",
    "print(supervised_y_train.shape, supervised_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "['Invasive' 'Invasive' 'Introduced' 'Indigenous' 'Invasive' 'Indigenous'\n",
      " 'Introduced' 'Introduced' 'Introduced' 'Invasive' 'Introduced'\n",
      " 'Introduced' 'Introduced' 'Introduced' 'Invasive' 'Invasive' 'Indigenous'\n",
      " 'Introduced' 'Indigenous' 'Introduced']\n",
      "[0.77777777777777779] 0.777777777778\n"
     ]
    }
   ],
   "source": [
    "X_test = sim_mat_test.T\n",
    "\n",
    "predict = clf.predict(X_test)\n",
    "\n",
    "print(predict[:20])\n",
    "# print(predict == np.array(cls_test))\n",
    "print(cls_test[:20])\n",
    "score = np.mean((predict == np.array(supervised_y_test))*1)\n",
    "avg_score.append(score)\n",
    "\n",
    "print(avg_score, np.mean(avg_score))\n",
    "\n",
    "# print 'Approximated similarity matrix: \\n'\n",
    "# test_and_score(supervised_X, supervised_y)\n",
    "# print 'Full similarity matrix: \\n'\n",
    "# test_and_score(full_supervised_X, supervised_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the results are very competitive.\n",
    "\n",
    "Now, we use cross validation method get another take on our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # cross validation method\n",
    "# # SVM\n",
    "\n",
    "# # tricks: shuffling data, cross validation, balanced classes, hyperparam tuning\n",
    "\n",
    "# def cv_test_and_score(supervised_X_train, supervised_y, c=1e14):\n",
    "#     scores = []\n",
    "#     param_vals = []\n",
    "    \n",
    "#     # shuffle the data\n",
    "#     # res_mat_shuff, cls_valid_shuff = sk.utils.shuffle(supervised_X, supervised_y, random_state=0)\n",
    "\n",
    "#     c = 10*c\n",
    "#     clf = SVC(C=c,kernel='poly', degree=2, coef0=0) #, gamma=i)\n",
    "\n",
    "#     score = sk.cross_validation.cross_val_score(clf, res_mat_shuff, cls_valid_shuff, cv=6) #, n_jobs=-1)\n",
    "#     # print('Prediction accuracy:', np.mean((prediction == np.array(cls_test))*1))\n",
    "#     #Coefficients used by the classifier\n",
    "\n",
    "#     scores.append(score)\n",
    "#     param_vals.append(i)\n",
    "\n",
    "#     print(scores)\n",
    "\n",
    "#     mn_scores = [np.mean(score) for score in scores]\n",
    "\n",
    "#     print('mean scores:', mn_scores)\n",
    "\n",
    "# print ('Approximated similarity matrix: \\n')\n",
    "# cv_test_and_score(supervised_X, supervised_y)\n",
    "# # print ('\\nFull similarity matrix: \\n')\n",
    "# # cv_test_and_score(full_supervised_X, supervised_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice that we included our testing data when creating the similiarity matrix, because we first create the matrix and then separate the data into train and test sets. This is somewhat unsatisfying, and very anonying if we want to do on the fly predictions. We have to recompute the simliarity matrix every time.\n",
    "\n",
    "We now try excluding the test data from computing the similarity matrix. Instead, we can compute the values for the test data afterwards. We then also need to project the similiarity values for the test data to the SVD space, before we can run SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
